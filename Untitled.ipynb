{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c169d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from os.path import isfile, join\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ec44d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv-python-4.10.0.84.tar.gz (95.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.1/95.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /Users/user/anaconda3/lib/python3.11/site-packages (from opencv-python) (1.24.3)\n",
      "Building wheels for collected packages: opencv-python\n",
      "  Building wheel for opencv-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for opencv-python: filename=opencv_python-4.10.0.84-cp311-cp311-macosx_10_16_x86_64.whl size=27295829 sha256=c43a7c79a020fdeedd18669da6366ca4e5baf7ca2fe33e4c058cca1136c9b428\n",
      "  Stored in directory: /Users/user/Library/Caches/pip/wheels/16/15/c8/aaf845de30a73df1a8a672bd9a27f5f2c62f8631fd54034492\n",
      "Successfully built opencv-python\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.10.0.84\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aee6d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_folder(filepath, limit=None):\n",
    "    onlyfiles = [f for f in listdir(filepath) if isfile(join(filepath,f)) and '.jpg' in f]\n",
    "    print(filepath)\n",
    "    print('Num pics in folder: {}'.format(len(onlyfiles)))\n",
    "    if limit != None:\n",
    "        num_files = limit\n",
    "        print('Only {} images being used'.format(num_files))\n",
    "    else:\n",
    "        num_files = len(onlyfiles)\n",
    "        print('All images being used')\n",
    "    images = np.empty(num_files, dtype=object)\n",
    "    for n in range(0, num_files):\n",
    "      images[n] = cv2.resize(cv2.imread(join(filepath,onlyfiles[n])), (1008, 756))\n",
    "      images[n] = cv2.cvtColor(images[n], cv2.COLOR_BGR2RGB)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "889dca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image, size):\n",
    "    image_list = []\n",
    "\n",
    "    #Find height, width of input image\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "\n",
    "    #Find out how many slices we can do for height and width based on size\n",
    "    height_slices = int(height / size)\n",
    "    width_slices = int(width / size)\n",
    "\n",
    "    #Set start and stop parameters for image cropping\n",
    "    h_start, h_stop = 0, size\n",
    "    w_start, w_stop = 0, size\n",
    "\n",
    "    #This for loop will\n",
    "    for i in range(height_slices):\n",
    "        for j in range(width_slices):\n",
    "            crop_img = image[h_start:h_stop, w_start:w_stop]\n",
    "            image_list.append(crop_img)\n",
    "            w_start += size\n",
    "            w_stop += size\n",
    "        h_start += size\n",
    "        h_stop += size\n",
    "        w_start = 0\n",
    "        w_stop = size\n",
    "\n",
    "    return image_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37acae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_list(image_list, size):\n",
    "    new_image_list = []\n",
    "    for i in image_list:\n",
    "        cropped_images = crop_image(i, size)\n",
    "        for j in cropped_images:\n",
    "            new_image_list.append(j)\n",
    "    return new_image_list\n",
    "\n",
    "def rotate_images_4x(image_list):\n",
    "    new_image_list = []\n",
    "\n",
    "    #Takes the image from the list, converts to NP array, flips it 90 degrees\n",
    "    #and saves it for 4 total images\n",
    "    for i in image_list:\n",
    "        img = np.array(i)\n",
    "        new_image_list.append(img)\n",
    "        for j in range(3):\n",
    "            img = np.rot90(img)\n",
    "            new_image_list.append(img)\n",
    "    return new_image_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a52b379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mirror_images(image_list):\n",
    "    new_image_list = []\n",
    "    for i in image_list:\n",
    "        img = np.array(i)\n",
    "        new_image_list.append(img)\n",
    "        img = np.fliplr(img)\n",
    "        new_image_list.append(img)\n",
    "    return new_image_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f41c5024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(image_list):\n",
    "    for i in range(len(image_list)):\n",
    "        cv2.imwrite('test_images/img_{}.jpg'.format(i), image_list[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30259485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_pipeline(folder, size):\n",
    "    images = load_image_folder(folder)\n",
    "    cropped = crop_image_list(images, size)\n",
    "    rot_crop = rotate_images_4x(cropped)\n",
    "    return mirror_images(rot_crop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ef0a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_total_pipeline(folder_list, size, limit=None):\n",
    "    X = 0\n",
    "    y = 0\n",
    "    for i in range(len(folder_list)):\n",
    "        if limit == None:\n",
    "            images = load_image_folder(folder_list[i])\n",
    "        else:\n",
    "            images = load_image_folder(folder_list[i], limit)\n",
    "        cropped = crop_image_list(images, size)\n",
    "        rot_crop = rotate_images_4x(cropped)\n",
    "        mirrored = mirror_images(rot_crop)\n",
    "        if type(X) == int:\n",
    "            X = np.array(mirrored)\n",
    "        else:\n",
    "            X = np.vstack((X, np.array(mirrored)))\n",
    "        if type(y) == int:\n",
    "            y = np.zeros(len(mirrored))\n",
    "        else:\n",
    "            y_arr = np.zeros(len(mirrored))\n",
    "            y_arr.fill(i)\n",
    "            y = np.append(y, y_arr)\n",
    "        print('X shape: {} -=-=-=-= y shape: {}'.format(X.shape, y.shape))\n",
    "    return np.array(X), y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f51e1547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@1.067] global loadsave.cpp:241 findDecoder imread_('maroon_bells.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(PATH)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#change the colors from BGR to RGB - this is unnecessary because imwrite flips it back\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# BGRflags = [flag for flag in dir(cv2) if flag.startswith('COLOR_BGR') ]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHeight:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m pixels\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWidth:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m pixels\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mChannels:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m image\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel at (0,0) [B,G,R]:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtuple\u001b[39m(image[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,:]))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata-type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m image\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    PATH = \"maroon_bells.jpg\"\n",
    "    image = cv2.imread(PATH)\n",
    "\n",
    "    #change the colors from BGR to RGB - this is unnecessary because imwrite flips it back\n",
    "    # BGRflags = [flag for flag in dir(cv2) if flag.startswith('COLOR_BGR') ]\n",
    "    # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    print(\"Height:\\t\\t%i pixels\\nWidth:\\t\\t%i pixels\\nChannels:\\t%i\" % image.shape)\n",
    "    print(\"pixel at (0,0) [B,G,R]:\\t[%i,%i,%i]\" % tuple(image[0,0,:]))\n",
    "    print(\"data-type: %s \" % image.dtype)\n",
    "\n",
    "    #This is how to resize the image\n",
    "    resized_image = cv2.resize(image, (100, 50))\n",
    "    # plt.imshow(resized_image)\n",
    "    # plt.show()\n",
    "\n",
    "    #Pipeline Test\n",
    "    l1 = []\n",
    "    l1.append(image)\n",
    "    rotated = rotate_images_4x(l1)\n",
    "    mirrored_rotated = mirror_images(rotated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84ff0be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from img_preprocess import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Conv2D\n",
    "# from keras.utils import np_utils\n",
    "from keras.utils import to_categorical as np_utils\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37de8a7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "prep_total_pipeline() got an unexpected keyword argument 'limit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m img_rows, img_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m50\u001b[39m \u001b[38;5;66;03m# input image dimensions\u001b[39;00m\n\u001b[1;32m      3\u001b[0m folder_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimgs/rockler_bubinga\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimgs/rockler_cherry\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimgs/cs_sycamore\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      4\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimgs/pine_douglas_fir_home_depot\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimgs/pine_sub\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimgs/red_oak_home_depot\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimgs/red_oak_sub\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimgs/rockler_poplar\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimgs/small_set_mahogany\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimgs/small_set_pine\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m X, y \u001b[38;5;241m=\u001b[39m prep_total_pipeline(folder_list, img_rows, limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      7\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      9\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: prep_total_pipeline() got an unexpected keyword argument 'limit'"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    img_rows, img_cols = 50, 50 # input image dimensions\n",
    "    folder_list = ['imgs/rockler_bubinga', 'imgs/rockler_cherry', 'imgs/cs_sycamore', \n",
    "                   'imgs/pine_douglas_fir_home_depot', 'imgs/pine_sub', 'imgs/red_oak_home_depot', 'imgs/red_oak_sub',\n",
    "                  'imgs/rockler_poplar', 'imgs/small_set_mahogany', 'imgs/small_set_pine']\n",
    "    X, y = prep_total_pipeline(folder_list, img_rows, limit=2)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    batch_size = 100\n",
    "    num_classes = len(folder_list)\n",
    "    num_epochs = 10\n",
    "    nb_filters = 32 # number of convolutional filters to use\n",
    "    pool_size = (2, 2) # size of pooling area for max pooling\n",
    "                       # decreases image size, and helps to avoid overfitting\n",
    "\n",
    "    if K.image_data_format() == 'th':\n",
    "        X_train = X_train.reshape(X_train.shape[0], X_train.shape[3], img_rows, img_cols)\n",
    "        X_test = X_test.reshape(X_test.shape[0], X_test.shape[3], img_rows, img_cols)\n",
    "        input_shape = (X_test.shape[3], img_rows, img_cols)\n",
    "    else:\n",
    "        X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, X_train.shape[3])\n",
    "        X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, X_test.shape[3])\n",
    "        input_shape = (img_rows, img_cols, X_test.shape[3])\n",
    "\n",
    "    # don't change conversion or normalization\n",
    "    X_train = X_train.astype('float32') # data was uint8 [0-255]\n",
    "    X_test = X_test.astype('float32')  # data was uint8 [0-255]\n",
    "    X_train /= 255 # normalizing (scaling from 0 to 1)\n",
    "    X_test /= 255  # normalizing (scaling from 0 to 1)\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print(X_train.shape[0], 'train samples')\n",
    "    print(X_test.shape[0], 'test samples')\n",
    "\n",
    "    # convert class vectors to binary class matrices (don't change)\n",
    "    Y_train = np_utils(y_train, num_classes) # cool\n",
    "    Y_test = np_utils(y_test, num_classes)   # cool * 2\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(nb_filters, (5, 5),\n",
    "                        padding='valid',\n",
    "                        input_shape=input_shape))\n",
    "    model.add(Activation('relu')) # Activation specification necessary for Conv2D and Dense layers\n",
    "    model.add(Conv2D(nb_filters, (3,3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(nb_filters, (2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size)) # decreases size, helps prevent overfitting\n",
    "    model.add(Dropout(0.5)) # zeros out some fraction of inputs, helps prevent overfitting\n",
    "\n",
    "    model.add(Flatten()) # necessary to flatten before going into conventional dense layer (keep layer)\n",
    "    print('Model flattened out to ', model.output_shape)\n",
    "\n",
    "    # now start a typical neural network\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(0.15))\n",
    "    model.add(Dense(num_classes)) # 10 final nodes (one for each class) (keep layer)\n",
    "    model.add(Activation('softmax')) # keep softmax at end to pick between classes 0-9\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # during fit process watch train and test error simultaneously\n",
    "    model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "              verbose=1, validation_data=(X_test, Y_test))\n",
    "\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1]) # this is the one we care about\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "859d6874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs/rockler_bubinga\n",
      "Num pics in folder: 11\n",
      "Only 11 images being used\n",
      "X shape: (26400, 50, 50, 3) -=-=-=-= y shape: (26400,)\n",
      "imgs/rockler_cherry\n",
      "Num pics in folder: 11\n",
      "Only 11 images being used\n",
      "X shape: (52800, 50, 50, 3) -=-=-=-= y shape: (52800,)\n",
      "imgs/cs_sycamore\n",
      "Num pics in folder: 11\n",
      "Only 11 images being used\n",
      "X shape: (79200, 50, 50, 3) -=-=-=-= y shape: (79200,)\n",
      "imgs/pine_douglas_fir_home_depot\n",
      "Num pics in folder: 11\n",
      "Only 11 images being used\n",
      "X shape: (105600, 50, 50, 3) -=-=-=-= y shape: (105600,)\n",
      "X_train shape: (84480, 50, 50, 3)\n",
      "84480 train samples\n",
      "21120 test samples\n",
      "Epoch 1/10\n",
      "845/845 [==============================] - 336s 395ms/step - loss: 0.7581 - accuracy: 0.6531 - val_loss: 0.5534 - val_accuracy: 0.7662\n",
      "Epoch 2/10\n",
      "845/845 [==============================] - 304s 360ms/step - loss: 0.5374 - accuracy: 0.7692 - val_loss: 0.6793 - val_accuracy: 0.7250\n",
      "Epoch 3/10\n",
      "845/845 [==============================] - 313s 371ms/step - loss: 0.4116 - accuracy: 0.8311 - val_loss: 0.5636 - val_accuracy: 0.7815\n",
      "Epoch 4/10\n",
      "845/845 [==============================] - 328s 388ms/step - loss: 0.3131 - accuracy: 0.8818 - val_loss: 0.2385 - val_accuracy: 0.9148\n",
      "Epoch 5/10\n",
      "845/845 [==============================] - 309s 366ms/step - loss: 0.2183 - accuracy: 0.9237 - val_loss: 0.1239 - val_accuracy: 0.9622\n",
      "Epoch 6/10\n",
      "845/845 [==============================] - 290s 342ms/step - loss: 0.1802 - accuracy: 0.9388 - val_loss: 0.0952 - val_accuracy: 0.9692\n",
      "Epoch 7/10\n",
      "845/845 [==============================] - 284s 336ms/step - loss: 0.2053 - accuracy: 0.9301 - val_loss: 0.1211 - val_accuracy: 0.9578\n",
      "Epoch 8/10\n",
      "845/845 [==============================] - 344s 407ms/step - loss: 0.1706 - accuracy: 0.9412 - val_loss: 0.0981 - val_accuracy: 0.9660\n",
      "Epoch 9/10\n",
      "845/845 [==============================] - 361s 427ms/step - loss: 0.1507 - accuracy: 0.9474 - val_loss: 0.1787 - val_accuracy: 0.9376\n",
      "Epoch 10/10\n",
      "845/845 [==============================] - 327s 387ms/step - loss: 0.1612 - accuracy: 0.9459 - val_loss: 0.1027 - val_accuracy: 0.9636\n",
      "Test score: 0.10273054987192154\n",
      "Test accuracy: 0.9636363387107849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to wood_species_model.h5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.utils import to_categorical as np_utils\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Activation, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "\n",
    "def load_image_folder(filepath, min_limit=2):\n",
    "    onlyfiles = [f for f in listdir(filepath) if isfile(join(filepath, f)) and '.jpg' in f]\n",
    "    print(filepath)\n",
    "    print('Num pics in folder: {}'.format(len(onlyfiles)))\n",
    "    \n",
    "    # Ensure limit is at least min_limit and at most the number of files available\n",
    "    num_files = max(min_limit, len(onlyfiles))\n",
    "    print('Only {} images being used'.format(num_files))\n",
    "    \n",
    "    images = np.empty(num_files, dtype=object)\n",
    "    for n in range(num_files):\n",
    "        images[n] = cv2.resize(cv2.imread(join(filepath, onlyfiles[n])), (1008, 756))\n",
    "        images[n] = cv2.cvtColor(images[n], cv2.COLOR_BGR2RGB)\n",
    "    return images\n",
    "\n",
    "def crop_image(image, size):\n",
    "    image_list = []\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "    height_slices = int(height / size)\n",
    "    width_slices = int(width / size)\n",
    "    h_start, h_stop = 0, size\n",
    "    w_start, w_stop = 0, size\n",
    "\n",
    "    for i in range(height_slices):\n",
    "        for j in range(width_slices):\n",
    "            crop_img = image[h_start:h_stop, w_start:w_stop]\n",
    "            image_list.append(crop_img)\n",
    "            w_start += size\n",
    "            w_stop += size\n",
    "        h_start += size\n",
    "        h_stop += size\n",
    "        w_start = 0\n",
    "        w_stop = size\n",
    "\n",
    "    return image_list\n",
    "\n",
    "def crop_image_list(image_list, size):\n",
    "    new_image_list = []\n",
    "    for i in image_list:\n",
    "        cropped_images = crop_image(i, size)\n",
    "        for j in cropped_images:\n",
    "            new_image_list.append(j)\n",
    "    return new_image_list\n",
    "\n",
    "def rotate_images_4x(image_list):\n",
    "    new_image_list = []\n",
    "    for i in image_list:\n",
    "        img = np.array(i)\n",
    "        new_image_list.append(img)\n",
    "        for j in range(3):\n",
    "            img = np.rot90(img)\n",
    "            new_image_list.append(img)\n",
    "    return new_image_list\n",
    "\n",
    "def mirror_images(image_list):\n",
    "    new_image_list = []\n",
    "    for i in image_list:\n",
    "        img = np.array(i)\n",
    "        new_image_list.append(img)\n",
    "        img = np.fliplr(img)\n",
    "        new_image_list.append(img)\n",
    "    return new_image_list\n",
    "\n",
    "def save_images(image_list):\n",
    "    for i in range(len(image_list)):\n",
    "        cv2.imwrite('test_images/img_{}.jpg'.format(i), image_list[i])\n",
    "\n",
    "def prep_pipeline(folder, size):\n",
    "    images = load_image_folder(folder)\n",
    "    cropped = crop_image_list(images, size)\n",
    "    rot_crop = rotate_images_4x(cropped)\n",
    "    return mirror_images(rot_crop)\n",
    "\n",
    "def prep_total_pipeline(folder_list, size, min_limit=2):\n",
    "    X = 0\n",
    "    y = 0\n",
    "    for i in range(len(folder_list)):\n",
    "        images = load_image_folder(folder_list[i], min_limit)\n",
    "        cropped = crop_image_list(images, size)\n",
    "        rot_crop = rotate_images_4x(cropped)\n",
    "        mirrored = mirror_images(rot_crop)\n",
    "        if type(X) == int:\n",
    "            X = np.array(mirrored)\n",
    "        else:\n",
    "            X = np.vstack((X, np.array(mirrored)))\n",
    "        if type(y) == int:\n",
    "            y = np.zeros(len(mirrored))\n",
    "        else:\n",
    "            y_arr = np.zeros(len(mirrored))\n",
    "            y_arr.fill(i)\n",
    "            y = np.append(y, y_arr)\n",
    "        print('X shape: {} -=-=-=-= y shape: {}'.format(X.shape, y.shape))\n",
    "    return np.array(X), y\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    img_rows, img_cols = 50, 50 # input image dimensions\n",
    "#     folder_list = ['imgs/rockler_bubinga', 'imgs/rockler_cherry', 'imgs/cs_sycamore', \n",
    "#                    'imgs/pine_douglas_fir_home_depot', 'imgs/pine_sub', 'imgs/red_oak_home_depot', 'imgs/red_oak_sub',\n",
    "#                    'imgs/rockler_poplar', 'imgs/small_set_mahogany', 'imgs/small_set_pine']\n",
    "    folder_list = ['imgs/rockler_bubinga', 'imgs/rockler_cherry', 'imgs/cs_sycamore', 'imgs/pine_douglas_fir_home_depot']\n",
    "    min_limit = 2\n",
    "    X, y = prep_total_pipeline(folder_list, img_rows, min_limit)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    batch_size = 100\n",
    "    num_classes = len(folder_list)\n",
    "    num_epochs = 10\n",
    "    nb_filters = 32 # number of convolutional filters to use\n",
    "    pool_size = (2, 2) # size of pooling area for max pooling\n",
    "\n",
    "    if K.image_data_format() == 'th':\n",
    "        X_train = X_train.reshape(X_train.shape[0], X_train.shape[3], img_rows, img_cols)\n",
    "        X_test = X_test.reshape(X_test.shape[0], X_test.shape[3], img_rows, img_cols)\n",
    "        input_shape = (X_test.shape[3], img_rows, img_cols)\n",
    "    else:\n",
    "        X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, X_train.shape[3])\n",
    "        X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, X_test.shape[3])\n",
    "        input_shape = (img_rows, img_cols, X_test.shape[3])\n",
    "\n",
    "    # don't change conversion or normalization\n",
    "    X_train = X_train.astype('float32') # data was uint8 [0-255]\n",
    "    X_test = X_test.astype('float32')  # data was uint8 [0-255]\n",
    "    X_train /= 255 # normalizing (scaling from 0 to 1)\n",
    "    X_test /= 255  # normalizing (scaling from 0 to 1)\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print(X_train.shape[0], 'train samples')\n",
    "    print(X_test.shape[0], 'test samples')\n",
    "\n",
    "    # convert class vectors to binary class matrices (don't change)\n",
    "    Y_train = np_utils(y_train, num_classes) # cool\n",
    "    Y_test = np_utils(y_test, num_classes)   # cool * 2\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(nb_filters, (5, 5),\n",
    "                        padding='valid',\n",
    "                        input_shape=input_shape))\n",
    "    model.add(Activation('relu')) # Activation specification necessary for Conv2D and Dense layers\n",
    "    model.add(Conv2D(nb_filters, (3,3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(nb_filters, (2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size)) # decreases size, helps prevent overfitting\n",
    "    model.add(Dropout(0.5)) # zeros out some fraction of inputs, helps prevent overfitting\n",
    "\n",
    "    model.add(Flatten()) # necessary to flatten before going into conventional dense layer\n",
    "    model.add(Dense(128)) # 128 arbitrary, but conventional\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes)) # 10 because 10 classes\n",
    "    model.add(Activation('softmax')) # softmax necessary for output\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "              verbose=1, validation_data=(X_test, Y_test))\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    \n",
    "    # Save the model to a file\n",
    "    model.save('wood_species_model.h5')\n",
    "    print(\"Model saved to wood_species_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e4370fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to wood_species_model.keras\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the existing model\n",
    "existing_model_path = 'wood_species_model.h5'\n",
    "model = load_model(existing_model_path)\n",
    "\n",
    "# Save the model in the new Keras format\n",
    "new_model_path = 'wood_species_model.keras'\n",
    "model.save(new_model_path)\n",
    "\n",
    "print(\"Model saved to\", new_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d65ddc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
